[2024-02-12T18:51:30.180+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:51:25.237261+00:00 [queued]>
[2024-02-12T18:51:30.200+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:51:25.237261+00:00 [queued]>
[2024-02-12T18:51:30.201+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2024-02-12T18:51:30.238+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-02-12 18:51:25.237261+00:00
[2024-02-12T18:51:30.249+0000] {standard_task_runner.py:60} INFO - Started process 74 to run task
[2024-02-12T18:51:30.260+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-02-12T18:51:25.237261+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpyqydr_11']
[2024-02-12T18:51:30.265+0000] {standard_task_runner.py:88} INFO - Job 12: Subtask reddit_extraction
[2024-02-12T18:51:30.374+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:51:25.237261+00:00 [running]> on host 9c5f8f1aa41b
[2024-02-12T18:51:30.572+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='ahmed Elsharkawy' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-02-12T18:51:25.237261+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-02-12T18:51:25.237261+00:00'
[2024-02-12T18:51:30.579+0000] {logging_mixin.py:188} INFO - connected to reddit!
[2024-02-12T18:51:32.000+0000] {logging_mixin.py:188} INFO - {'id': '1aofpbr', 'title': 'What we learned after running Airflow on Kubernetes for 2 years', 'score': 157, 'num_comments': 14, 'author': Redditor(name='UpvoteBeast'), 'created_utc': 1707678469.0, 'url': 'https://api.daily.dev/r/HAWJyvDVy', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.007+0000] {logging_mixin.py:188} INFO - {'id': '1aofkv9', 'title': '[Updated] Personal End-End ETL data pipeline(GCP, SPARK, AIRFLOW, TERRAFORM, DOCKER, DL, D3.JS)', 'score': 56, 'num_comments': 14, 'author': Redditor(name='AffectionateEmu8146'), 'created_utc': 1707678149.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aofkv9/updated_personal_endend_etl_data_pipelinegcp/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.010+0000] {logging_mixin.py:188} INFO - {'id': '1aopq7t', 'title': 'Pandas, high coupling and single responsibility principle in data pipelines', 'score': 33, 'num_comments': 15, 'author': Redditor(name='Confident_Watch8207'), 'created_utc': 1707706242.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aopq7t/pandas_high_coupling_and_single_responsibility/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.013+0000] {logging_mixin.py:188} INFO - {'id': '1aonpuf', 'title': 'Isnâ€™t DBT an unecessary layer if you use BigQuery ?', 'score': 29, 'num_comments': 63, 'author': Redditor(name='VegetableFan6622'), 'created_utc': 1707699762.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aonpuf/isnt_dbt_an_unecessary_layer_if_you_use_bigquery/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.014+0000] {logging_mixin.py:188} INFO - {'id': '1aos0o3', 'title': 'How do I write a data pipeline (ETL) ?', 'score': 13, 'num_comments': 3, 'author': Redditor(name='c0m94d3'), 'created_utc': 1707713997.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aos0o3/how_do_i_write_a_data_pipeline_etl/', 'over_18': False, 'edited': 1707742199.0, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.015+0000] {logging_mixin.py:188} INFO - {'id': '1ap40zg', 'title': 'Data Engineering vs Data Engineering', 'score': 18, 'num_comments': 9, 'author': Redditor(name='apple_pie_52'), 'created_utc': 1707755384.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.017+0000] {logging_mixin.py:188} INFO - {'id': '1ap2xop', 'title': 'Thoughts on OneTable/XTable?', 'score': 10, 'num_comments': 4, 'author': Redditor(name='Data_cruncher'), 'created_utc': 1707752677.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap2xop/thoughts_on_onetablextable/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.020+0000] {logging_mixin.py:188} INFO - {'id': '1aopf7d', 'title': 'What technology would you use if you had a txt extract of a customer data with 16 million rows, and 30 columns and had to make a "user friendly" filtering system?', 'score': 10, 'num_comments': 15, 'author': Redditor(name='Candid94'), 'created_utc': 1707705251.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aopf7d/what_technology_would_you_use_if_you_had_a_txt/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.021+0000] {logging_mixin.py:188} INFO - {'id': '1aol2vr', 'title': 'GenAI and DE', 'score': 6, 'num_comments': 5, 'author': Redditor(name='Wise_Shop6419'), 'created_utc': 1707692116.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aol2vr/genai_and_de/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.026+0000] {logging_mixin.py:188} INFO - {'id': '1ap269w', 'title': 'Advice for someone trying to transition into data engineering from academia?', 'score': 8, 'num_comments': 7, 'author': Redditor(name='ianmgull'), 'created_utc': 1707750659.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap269w/advice_for_someone_trying_to_transition_into_data/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.027+0000] {logging_mixin.py:188} INFO - {'id': '1aoygvj', 'title': 'Best approach to 3rd ingest data into Redshift / Snowflake?', 'score': 6, 'num_comments': 3, 'author': Redditor(name='Peivol'), 'created_utc': 1707739412.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoygvj/best_approach_to_3rd_ingest_data_into_redshift/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.029+0000] {logging_mixin.py:188} INFO - {'id': '1aoenig', 'title': 'Dataverse', 'score': 7, 'num_comments': 17, 'author': Redditor(name='Ancient-Entry-6436'), 'created_utc': 1707675829.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoenig/dataverse/', 'over_18': False, 'edited': 1707699634.0, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.030+0000] {logging_mixin.py:188} INFO - {'id': '1aovhqs', 'title': 'Is dbt still relevant on Snowflake with Dynamic Tables?', 'score': 5, 'num_comments': 3, 'author': Redditor(name='nydasco'), 'created_utc': 1707727196.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aovhqs/is_dbt_still_relevant_on_snowflake_with_dynamic/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.031+0000] {logging_mixin.py:188} INFO - {'id': '1ap3flw', 'title': 'Managing SQL Table Changes', 'score': 4, 'num_comments': 2, 'author': Redditor(name='machinegunke11y'), 'created_utc': 1707753939.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap3flw/managing_sql_table_changes/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.032+0000] {logging_mixin.py:188} INFO - {'id': '1aoz9ic', 'title': 'How did you choose your BI setup?', 'score': 6, 'num_comments': 6, 'author': Redditor(name='AdImaginary8024'), 'created_utc': 1707742134.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoz9ic/how_did_you_choose_your_bi_setup/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.034+0000] {logging_mixin.py:188} INFO - {'id': '1aoy4oo', 'title': 'I developed a cool new LLM agent that helps with investigating and resolving alerts faster', 'score': 4, 'num_comments': 1, 'author': Redditor(name='Old_Cauliflower6316'), 'created_utc': 1707738198.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoy4oo/i_developed_a_cool_new_llm_agent_that_helps_with/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.035+0000] {logging_mixin.py:188} INFO - {'id': '1aoue6j', 'title': 'How to manage Hot and Cold Tables for Streaming Data', 'score': 2, 'num_comments': 1, 'author': Redditor(name='HousingStriking3770'), 'created_utc': 1707722542.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoue6j/how_to_manage_hot_and_cold_tables_for_streaming/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.039+0000] {logging_mixin.py:188} INFO - {'id': '1ap22x7', 'title': 'Study advise for DP-203 Aspirants.', 'score': 4, 'num_comments': 1, 'author': Redditor(name='Vikinghehe'), 'created_utc': 1707750407.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap22x7/study_advise_for_dp203_aspirants/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.040+0000] {logging_mixin.py:188} INFO - {'id': '1aouc2b', 'title': 'Viable starting paths to DE?', 'score': 3, 'num_comments': 1, 'author': Redditor(name='Kylerhanley'), 'created_utc': 1707722313.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aouc2b/viable_starting_paths_to_de/', 'over_18': False, 'edited': 1707722769.0, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.041+0000] {logging_mixin.py:188} INFO - {'id': '1aohyqj', 'title': 'Nsx VMware', 'score': 3, 'num_comments': 0, 'author': Redditor(name='OldParticular2326'), 'created_utc': 1707684144.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aohyqj/nsx_vmware/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.043+0000] {logging_mixin.py:188} INFO - {'id': '1ap0ygf', 'title': 'dlt (Data Load Tool) adds Databricks and Azure Synapse destinations', 'score': 1, 'num_comments': 0, 'author': Redditor(name='Thinker_Assignment'), 'created_utc': 1707747301.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap0ygf/dlt_data_load_tool_adds_databricks_and_azure/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.044+0000] {logging_mixin.py:188} INFO - {'id': '1aoy0ku', 'title': 'Loading big query data to spark dataframe.', 'score': 2, 'num_comments': 1, 'author': Redditor(name='Dr_Fida'), 'created_utc': 1707737768.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoy0ku/loading_big_query_data_to_spark_dataframe/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.045+0000] {logging_mixin.py:188} INFO - {'id': '1aogoyd', 'title': 'A Kafka Connect Single Message Transform (SMT) that enables you to append the record key to the value as a named field', 'score': 2, 'num_comments': 0, 'author': Redditor(name='eladleev'), 'created_utc': 1707680980.0, 'url': 'https://github.com/EladLeev/KeyToField-smt', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.046+0000] {logging_mixin.py:188} INFO - {'id': '1aogc7m', 'title': 'Data API', 'score': 2, 'num_comments': 1, 'author': Redditor(name='Hot_Map_7868'), 'created_utc': 1707680103.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aogc7m/data_api/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.047+0000] {logging_mixin.py:188} INFO - {'id': '1ap5xup', 'title': 'Securing Patient Data in Pharma/Healthcare.', 'score': 2, 'num_comments': 2, 'author': Redditor(name='throwaway_112801'), 'created_utc': 1707759979.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap5xup/securing_patient_data_in_pharmahealthcare/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.049+0000] {logging_mixin.py:188} INFO - {'id': '1ap58sg', 'title': 'Rethinking Serverless: The Price of Convenience', 'score': 1, 'num_comments': 0, 'author': Redditor(name='sync_jeff'), 'created_utc': 1707758293.0, 'url': 'https://medium.com/sync-computing/rethinking-serverless-the-price-of-convenience-9b9e29549d3b', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.050+0000] {logging_mixin.py:188} INFO - {'id': '1ap55q1', 'title': 'There is a way to block a catalog access on databricks if connected to powerbi/tableau?', 'score': 1, 'num_comments': 0, 'author': Redditor(name='raffapaiva'), 'created_utc': 1707758092.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap55q1/there_is_a_way_to_block_a_catalog_access_on/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.053+0000] {logging_mixin.py:188} INFO - {'id': '1ap379e', 'title': 'Call snowpark stored procedures from power automate', 'score': 1, 'num_comments': 0, 'author': Redditor(name='asud_w_asud'), 'created_utc': 1707753361.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap379e/call_snowpark_stored_procedures_from_power/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.055+0000] {logging_mixin.py:188} INFO - {'id': '1aoxq13', 'title': 'SAP HANA jobs', 'score': 1, 'num_comments': 3, 'author': Redditor(name='Adorable_Finance3027'), 'created_utc': 1707736655.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoxq13/sap_hana_jobs/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.057+0000] {logging_mixin.py:188} INFO - {'id': '1aowiur', 'title': 'The dbt Materialzed_views Materialization', 'score': 1, 'num_comments': 0, 'author': Redditor(name='etsh_96'), 'created_utc': 1707731795.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aowiur/the_dbt_materialzed_views_materialization/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.061+0000] {logging_mixin.py:188} INFO - {'id': '1aolwjt', 'title': 'ORM vs query builder', 'score': 1, 'num_comments': 2, 'author': Redditor(name='colet_te'), 'created_utc': 1707694360.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aolwjt/orm_vs_query_builder/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.062+0000] {logging_mixin.py:188} INFO - {'id': '1aolkgc', 'title': 'Experience with Abbott?', 'score': 1, 'num_comments': 4, 'author': Redditor(name='Terrible_Mud5318'), 'created_utc': 1707693429.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aolkgc/experience_with_abbott/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.064+0000] {logging_mixin.py:188} INFO - {'id': '1ap64p9', 'title': 'Freshers into Data Engineering, Is hiriing going on or not', 'score': 0, 'num_comments': 3, 'author': Redditor(name='ganwaniKamal'), 'created_utc': 1707760439.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap64p9/freshers_into_data_engineering_is_hiriing_going/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.065+0000] {logging_mixin.py:188} INFO - {'id': '1ap2asm', 'title': 'snowflake - tracking changes', 'score': 0, 'num_comments': 0, 'author': Redditor(name='87keicam'), 'created_utc': 1707750992.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap2asm/snowflake_tracking_changes/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.066+0000] {logging_mixin.py:188} INFO - {'id': '1aoj4m3', 'title': 'Complex data transformations in azure', 'score': 0, 'num_comments': 6, 'author': Redditor(name='lschozar'), 'created_utc': 1707687098.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoj4m3/complex_data_transformations_in_azure/', 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-02-12T18:51:32.105+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 12, in reddit_pipeline
    load_data_to_csv(posts_df,OUTPUT_PATH)
  File "/opt/airflow/etls/reddit_etl.py", line 53, in load_data_to_csv
    data.to_csv(path,index=False)
  File "/home/airflow/.local/lib/python3.10/site-packages/pandas/core/generic.py", line 3902, in to_csv
    return DataFrameRenderer(formatter).to_csv(
  File "/home/airflow/.local/lib/python3.10/site-packages/pandas/io/formats/format.py", line 1152, in to_csv
    csv_formatter.save()
  File "/home/airflow/.local/lib/python3.10/site-packages/pandas/io/formats/csvs.py", line 247, in save
    with get_handle(
  File "/home/airflow/.local/lib/python3.10/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
IsADirectoryError: [Errno 21] Is a directory: '/opt/airflow/data/output'
[2024-02-12T18:51:32.155+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20240212T185125, start_date=20240212T185130, end_date=20240212T185132
[2024-02-12T18:51:32.192+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 12 for task reddit_extraction ([Errno 21] Is a directory: '/opt/airflow/data/output'; 74)
[2024-02-12T18:51:32.249+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-12T18:51:32.319+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
