[2024-02-12T18:20:05.961+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:20:02.800725+00:00 [queued]>
[2024-02-12T18:20:05.978+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:20:02.800725+00:00 [queued]>
[2024-02-12T18:20:05.980+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2024-02-12T18:20:06.006+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-02-12 18:20:02.800725+00:00
[2024-02-12T18:20:06.016+0000] {standard_task_runner.py:60} INFO - Started process 60 to run task
[2024-02-12T18:20:06.023+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-02-12T18:20:02.800725+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpqxufgwte']
[2024-02-12T18:20:06.028+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask reddit_extraction
[2024-02-12T18:20:06.126+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-02-12T18:20:02.800725+00:00 [running]> on host 9c5f8f1aa41b
[2024-02-12T18:20:06.322+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='ahmed Elsharkawy' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-02-12T18:20:02.800725+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-02-12T18:20:02.800725+00:00'
[2024-02-12T18:20:06.329+0000] {logging_mixin.py:188} INFO - connected to reddit!
[2024-02-12T18:20:07.697+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_c0f70ajh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What we learned after running Airflow on Kubernetes for 2 years', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aofpbr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'ups': 158, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 158, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/NRo17tiAO9hX-xEB-0yUrdp8w5CHKVLe_oFL_qX5B50.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1707678469.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'api.daily.dev', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://api.daily.dev/r/HAWJyvDVy', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?auto=webp&s=437d341e11e658f5b720f1602f9bb1a61c0789af', 'width': 1024, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?width=108&crop=smart&auto=webp&s=cc32c2c535eddb86279543c58fff194a4d4e68b7', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?width=216&crop=smart&auto=webp&s=8f7fbc05f0c633fe410fa9b1116bcb5b28dff2d5', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?width=320&crop=smart&auto=webp&s=6193685e5d95ed6ab4bf61fc5f7cf444848c973e', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?width=640&crop=smart&auto=webp&s=2dabe2feab2d4684d98c93652704ef5db8c53938', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/JM8TAVP2x3Lg1S8Fx0XXjWfC9Jn16ZSotxFMrpvf8FY.jpg?width=960&crop=smart&auto=webp&s=7690b6333d0278538cdd7c37a4f02adf556a47d9', 'width': 960, 'height': 960}], 'variants': {}, 'id': '222hXEHjEQgP5OrY9yu26A_RbxQRGXyluMOeD7ebrNA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1aofpbr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='UpvoteBeast'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aofpbr/what_we_learned_after_running_airflow_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://api.daily.dev/r/HAWJyvDVy', 'subreddit_subscribers': 160218, 'created_utc': 1707678469.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.699+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Github repo:[https://github.com/Zzdragon66/university-reddit-data-dashboard](https://github.com/Zzdragon66/university-reddit-data-dashboard).\n\nHey everyone, here's an update on the previous project. I would really appreciate any suggestions for improvement. Thank you!\n\n## Features\n\n1. The project is entirely hosted on the Google Cloud Platform\n2. This project is ***horizontal scalable***. The scraping workload is evenly distributed across the computer engines(VM). Data manipulation is done through the Spark cluster(Google dataproc), where by increasing the worker node, the workload will be distributed across and finished more quickly.\n3. The data transformation phase incorporates ***deep learning*** techniques to enhance analysis and insights.\n4. For data visualization, the project utilizes D3.js to create graphical representations.\n\n## Project Structure\n\n&#x200B;\n\nhttps://preview.redd.it/ew1cjp8870ic1.png?width=9426&format=png&auto=webp&s=502a9d668f0f7453f770cd9513ac33c041309e7a\n\n## Data Dashboard Examples\n\n## Example Local Dashboard(D3.js)\n\nhttps://preview.redd.it/fdhivgm970ic1.png?width=4038&format=png&auto=webp&s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7\n\n## Example Google Looker Studio Data Dashboard\n\n[Looker Studio Data Dashboard](https://lookerstudio.google.com/s/hEfY-Q6G4Fo)\n\n&#x200B;\n\nhttps://preview.redd.it/m5imqa5b70ic1.png?width=2886&format=png&auto=webp&s=32ea902c25e4f16b55da2085912d7585c743c6c5\n\n## Tools\n\n1. Python\n   1. PyTorch\n   2. Google Cloud Client Library\n   3. Huggingface\n2. Spark(*Data manipulation*)\n3. Apache Airflow(*Data orchestration*)\n   1. Dynamic DAG generation\n   2. Xcom\n   3. Variables\n   4. TaskGroup\n4. Google Cloud Platform\n   1. Computer Engine(*VM & Deep learning*)\n   2. Dataproc (*Spark*)\n   3. Bigquery (*SQL*)\n   4. Cloud Storage (*Data Storage*)\n   5. Looker Studio (*Data visualization*)\n   6. VPC Network and Firewall Rules\n5. Terraform(*Cloud Infrastructure Management*)\n6. Docker(*containerization*) and Dockerhub(*Distribute container images*)\n7. SQL(*Data Manipulation*)\n8. Javascript\n   1. D3.js for data visualization\n9. Makefile", 'author_fullname': 't2_5igde9z6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Updated] Personal End-End ETL data pipeline(GCP, SPARK, AIRFLOW, TERRAFORM, DOCKER, DL, D3.JS)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ew1cjp8870ic1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 39, 'x': 108, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=108&crop=smart&auto=webp&s=c8ed61f4421021c1f25fb50acff0758856180878'}, {'y': 78, 'x': 216, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=216&crop=smart&auto=webp&s=adce36a6133b6aa9557b327bda71926d921274e9'}, {'y': 116, 'x': 320, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=320&crop=smart&auto=webp&s=7244b9b08d87f76fa088e34766fc34b6c4c76c25'}, {'y': 232, 'x': 640, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=640&crop=smart&auto=webp&s=09fd658166fa41a4820a279c0fff772c40175992'}, {'y': 348, 'x': 960, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=960&crop=smart&auto=webp&s=9f02e11ed2b9f98fa7ef1fbcb554d2549b16e164'}, {'y': 391, 'x': 1080, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=1080&crop=smart&auto=webp&s=f8a79215dfe71cfdded2084a53802170f7d05911'}], 's': {'y': 3420, 'x': 9426, 'u': 'https://preview.redd.it/ew1cjp8870ic1.png?width=9426&format=png&auto=webp&s=502a9d668f0f7453f770cd9513ac33c041309e7a'}, 'id': 'ew1cjp8870ic1'}, 'fdhivgm970ic1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 50, 'x': 108, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=108&crop=smart&auto=webp&s=35279c5206c5b6f019c7feb73f91b6c09af4f676'}, {'y': 101, 'x': 216, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=216&crop=smart&auto=webp&s=87c0295eae64831dd434292ac93888a5e89bebd8'}, {'y': 150, 'x': 320, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=320&crop=smart&auto=webp&s=eb800ba4578a4854d6d78d186cf307eba2a0fa0b'}, {'y': 300, 'x': 640, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=640&crop=smart&auto=webp&s=b3b2ef54596bbd3ac6aaa259bf3be4b78d72aa66'}, {'y': 450, 'x': 960, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=960&crop=smart&auto=webp&s=b724b4637b5a57e8a88e65fc2b64cd261c0d0678'}, {'y': 507, 'x': 1080, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=1080&crop=smart&auto=webp&s=5d73ea5df080d681f51125678d8bceac869be5e9'}], 's': {'y': 1896, 'x': 4038, 'u': 'https://preview.redd.it/fdhivgm970ic1.png?width=4038&format=png&auto=webp&s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7'}, 'id': 'fdhivgm970ic1'}, 'm5imqa5b70ic1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 80, 'x': 108, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=108&crop=smart&auto=webp&s=616a7a46aa150b08e4ad8e053d0b73204e4a727c'}, {'y': 160, 'x': 216, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=216&crop=smart&auto=webp&s=4918166e170095d7259383d29303f109235068e0'}, {'y': 237, 'x': 320, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=320&crop=smart&auto=webp&s=503e9376b64a3cc6f2de6eeb2677aae5509d5a76'}, {'y': 474, 'x': 640, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=640&crop=smart&auto=webp&s=30d7bab717117438c5b6b06644d300b3413bcd9d'}, {'y': 711, 'x': 960, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=960&crop=smart&auto=webp&s=5bae71ea60fea5abcff78eed106be339a6711aea'}, {'y': 800, 'x': 1080, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=1080&crop=smart&auto=webp&s=17a5423322213e3f8d34cedb40049a4b09556c53'}], 's': {'y': 2138, 'x': 2886, 'u': 'https://preview.redd.it/m5imqa5b70ic1.png?width=2886&format=png&auto=webp&s=32ea902c25e4f16b55da2085912d7585c743c6c5'}, 'id': 'm5imqa5b70ic1'}}, 'name': 't3_1aofkv9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'ups': 57, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 57, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/7LTToHvYuUPEFBx2GEoigXqk54N8JrRlNtK8E4IJUSA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1707678149.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Github repo:<a href="https://github.com/Zzdragon66/university-reddit-data-dashboard">https://github.com/Zzdragon66/university-reddit-data-dashboard</a>.</p>\n\n<p>Hey everyone, here&#39;s an update on the previous project. I would really appreciate any suggestions for improvement. Thank you!</p>\n\n<h2>Features</h2>\n\n<ol>\n<li>The project is entirely hosted on the Google Cloud Platform</li>\n<li>This project is <strong><em>horizontal scalable</em></strong>. The scraping workload is evenly distributed across the computer engines(VM). Data manipulation is done through the Spark cluster(Google dataproc), where by increasing the worker node, the workload will be distributed across and finished more quickly.</li>\n<li>The data transformation phase incorporates <strong><em>deep learning</em></strong> techniques to enhance analysis and insights.</li>\n<li>For data visualization, the project utilizes D3.js to create graphical representations.</li>\n</ol>\n\n<h2>Project Structure</h2>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/ew1cjp8870ic1.png?width=9426&amp;format=png&amp;auto=webp&amp;s=502a9d668f0f7453f770cd9513ac33c041309e7a">https://preview.redd.it/ew1cjp8870ic1.png?width=9426&amp;format=png&amp;auto=webp&amp;s=502a9d668f0f7453f770cd9513ac33c041309e7a</a></p>\n\n<h2>Data Dashboard Examples</h2>\n\n<h2>Example Local Dashboard(D3.js)</h2>\n\n<p><a href="https://preview.redd.it/fdhivgm970ic1.png?width=4038&amp;format=png&amp;auto=webp&amp;s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7">https://preview.redd.it/fdhivgm970ic1.png?width=4038&amp;format=png&amp;auto=webp&amp;s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7</a></p>\n\n<h2>Example Google Looker Studio Data Dashboard</h2>\n\n<p><a href="https://lookerstudio.google.com/s/hEfY-Q6G4Fo">Looker Studio Data Dashboard</a></p>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/m5imqa5b70ic1.png?width=2886&amp;format=png&amp;auto=webp&amp;s=32ea902c25e4f16b55da2085912d7585c743c6c5">https://preview.redd.it/m5imqa5b70ic1.png?width=2886&amp;format=png&amp;auto=webp&amp;s=32ea902c25e4f16b55da2085912d7585c743c6c5</a></p>\n\n<h2>Tools</h2>\n\n<ol>\n<li>Python\n\n<ol>\n<li>PyTorch</li>\n<li>Google Cloud Client Library</li>\n<li>Huggingface</li>\n</ol></li>\n<li>Spark(<em>Data manipulation</em>)</li>\n<li>Apache Airflow(<em>Data orchestration</em>)\n\n<ol>\n<li>Dynamic DAG generation</li>\n<li>Xcom</li>\n<li>Variables</li>\n<li>TaskGroup</li>\n</ol></li>\n<li>Google Cloud Platform\n\n<ol>\n<li>Computer Engine(<em>VM &amp; Deep learning</em>)</li>\n<li>Dataproc (<em>Spark</em>)</li>\n<li>Bigquery (<em>SQL</em>)</li>\n<li>Cloud Storage (<em>Data Storage</em>)</li>\n<li>Looker Studio (<em>Data visualization</em>)</li>\n<li>VPC Network and Firewall Rules</li>\n</ol></li>\n<li>Terraform(<em>Cloud Infrastructure Management</em>)</li>\n<li>Docker(<em>containerization</em>) and Dockerhub(<em>Distribute container images</em>)</li>\n<li>SQL(<em>Data Manipulation</em>)</li>\n<li>Javascript\n\n<ol>\n<li>D3.js for data visualization</li>\n</ol></li>\n<li>Makefile</li>\n</ol>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?auto=webp&s=3f8dcaa15caacf8384bd61463b8c28a99856354f', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=108&crop=smart&auto=webp&s=fb4758bf0ed8f28e44bb1f7a697cfc60982b52a7', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=216&crop=smart&auto=webp&s=7411b6cf2b7622b35d0a27bf9983a30c52bb8f4e', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=320&crop=smart&auto=webp&s=4e84f77a5a72f7f9dcc12a1d540aa23dfecf99bf', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=640&crop=smart&auto=webp&s=06bb89980164a3ed06701c49c2aeb4649388e0ba', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=960&crop=smart&auto=webp&s=692d0fc68330ed18aaecc868646b87bcfd63174a', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/zldZHCckAmmqploRGhSZ9iZrsrFW7iQ9HTaF9W2Xw5Q.jpg?width=1080&crop=smart&auto=webp&s=0bd366ff7d607b1ec05396474d7cae6ff0e8a296', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'tCKgBogZ9tNepZfFFQ21QUDX0HCTKjVu8NJgPg_h-x8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1aofkv9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AffectionateEmu8146'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aofkv9/updated_personal_endend_etl_data_pipelinegcp/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aofkv9/updated_personal_endend_etl_data_pipelinegcp/', 'subreddit_subscribers': 160218, 'created_utc': 1707678149.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.701+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello. Everyone. I\'m a data scientist who also does Data Engineering at work depending on the business needs. Pandas is the tool I use the most for data pipelines (if my task doesn\'t involve big data), and I always face the same problem in the functions I write for the different steps in the pipeline:\n\nHuge functions with only 1 responsibility and too many steps which end up looking like huge blocks of pandas for accomplishing that one task. The thing is that if I start splitting that huge function into smaller parts, then those functions would only exist for that larger function to use.\n\nSo, I ultimately keep those larger functions because I\'m not sure whether it is ok to have functions that are only used once for just one "consumer"\n\nHow could I deal with this problem in a way that\'s easier to read, doesn\'t involve huge walls of pandas and having functions that are highly coupled to one another, and pretty much exist for a "greater good" (i dont know what else to call it lol)\n\nNote: I\'d like to know if there are common design patterns for this (maybe using OOP)', 'author_fullname': 't2_kvl7dwa6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pandas, high coupling and single responsibility principle in data pipelines', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aopq7t', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707706242.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello. Everyone. I&#39;m a data scientist who also does Data Engineering at work depending on the business needs. Pandas is the tool I use the most for data pipelines (if my task doesn&#39;t involve big data), and I always face the same problem in the functions I write for the different steps in the pipeline:</p>\n\n<p>Huge functions with only 1 responsibility and too many steps which end up looking like huge blocks of pandas for accomplishing that one task. The thing is that if I start splitting that huge function into smaller parts, then those functions would only exist for that larger function to use.</p>\n\n<p>So, I ultimately keep those larger functions because I&#39;m not sure whether it is ok to have functions that are only used once for just one &quot;consumer&quot;</p>\n\n<p>How could I deal with this problem in a way that&#39;s easier to read, doesn&#39;t involve huge walls of pandas and having functions that are highly coupled to one another, and pretty much exist for a &quot;greater good&quot; (i dont know what else to call it lol)</p>\n\n<p>Note: I&#39;d like to know if there are common design patterns for this (maybe using OOP)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aopq7t', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Confident_Watch8207'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aopq7t/pandas_high_coupling_and_single_responsibility/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aopq7t/pandas_high_coupling_and_single_responsibility/', 'subreddit_subscribers': 160218, 'created_utc': 1707706242.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.702+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I mean BigQuery can be called with the API, so I can just drop the config on a for and call the jobs when I need it orchestrated in Composer.\n\nI know there is the lineage management but I’m not a fan of monolithic trees accumulated somewhere. BQ tables creation often occurs after transforming pipelines in Java or Python.\n\nFinally, I had to write specific functions to «\xa0trick\xa0» DBT doing the optimization I need (which I could have done directly in SQL by BQ API calls).\n\nDoes someone has an experience with it ?\nI’m not sure what I should think about it.\n\nMy main fear is that people tend to accumulate queries on a single point and make unecessary and dangerous dependencies between tables while not cleaning the mess. Yes a tool is as good as you use it but I feel like the tool itself encourages you to fell on the trap…', 'author_fullname': 't2_ly1f1zyul', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Isn’t DBT an unecessary layer if you use BigQuery ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aonpuf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 29, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707699762.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I mean BigQuery can be called with the API, so I can just drop the config on a for and call the jobs when I need it orchestrated in Composer.</p>\n\n<p>I know there is the lineage management but I’m not a fan of monolithic trees accumulated somewhere. BQ tables creation often occurs after transforming pipelines in Java or Python.</p>\n\n<p>Finally, I had to write specific functions to «\xa0trick\xa0» DBT doing the optimization I need (which I could have done directly in SQL by BQ API calls).</p>\n\n<p>Does someone has an experience with it ?\nI’m not sure what I should think about it.</p>\n\n<p>My main fear is that people tend to accumulate queries on a single point and make unecessary and dangerous dependencies between tables while not cleaning the mess. Yes a tool is as good as you use it but I feel like the tool itself encourages you to fell on the trap…</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aonpuf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='VegetableFan6622'), 'discussion_type': None, 'num_comments': 63, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aonpuf/isnt_dbt_an_unecessary_layer_if_you_use_bigquery/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aonpuf/isnt_dbt_an_unecessary_layer_if_you_use_bigquery/', 'subreddit_subscribers': 160218, 'created_utc': 1707699762.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.704+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm working on a project, the current phase involves parsing PDFs (tables, headers) and extracting the and transforming the data into JSON objects that I am currently dumping into a text files. For reference, the PDFs are 1000+ pages, and produce two lists of JSON objects, one has a size of around 7000+, and other only <1000. The parsing takes about 5-7 minutes for each PDF. There are about 20-30 PDFs that are this large.\n\nI should mention, I already have most of the parser, and need a no-code/code open-source tool to move the data from the parsed PDFs to a real database.\n\nMy goal is to create a data pipeline... Or an ETL (I think that's called a data pipeline atleast), that reads the JSON objects (in parallel maybe?) and make some very tiny changes (stripping, splitting strings) on the JSON and dump into a DB. I can use SQLAlchemy as an ORM, but I'll later be writing an [ASP.NET](https://ASP.NET) WebAPI, so can I use Entity Framework for the end of the pipeline?\n\nI also have a daemonized/cron update service (script) that looks for new PDFs, downloads them, I need to feed these to the parser and then the pipeline. How do I achieve this workflow?\n\nThis is my first data engineering project, so I have no idea what tools could help me here (Airflow, Kafka, ...?). How do I get started?", 'author_fullname': 't2_45b66kd8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do I write a data pipeline (ETL) ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aos0o3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1707742199.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707713997.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working on a project, the current phase involves parsing PDFs (tables, headers) and extracting the and transforming the data into JSON objects that I am currently dumping into a text files. For reference, the PDFs are 1000+ pages, and produce two lists of JSON objects, one has a size of around 7000+, and other only &lt;1000. The parsing takes about 5-7 minutes for each PDF. There are about 20-30 PDFs that are this large.</p>\n\n<p>I should mention, I already have most of the parser, and need a no-code/code open-source tool to move the data from the parsed PDFs to a real database.</p>\n\n<p>My goal is to create a data pipeline... Or an ETL (I think that&#39;s called a data pipeline atleast), that reads the JSON objects (in parallel maybe?) and make some very tiny changes (stripping, splitting strings) on the JSON and dump into a DB. I can use SQLAlchemy as an ORM, but I&#39;ll later be writing an <a href="https://ASP.NET">ASP.NET</a> WebAPI, so can I use Entity Framework for the end of the pipeline?</p>\n\n<p>I also have a daemonized/cron update service (script) that looks for new PDFs, downloads them, I need to feed these to the parser and then the pipeline. How do I achieve this workflow?</p>\n\n<p>This is my first data engineering project, so I have no idea what tools could help me here (Airflow, Kafka, ...?). How do I get started?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/RvqZTha_jpsuzZWXfSdwZNHZ_Qvk62R3yl7YvkufrqI.jpg?auto=webp&s=dac8ba253829e5b1a0d52a07b3d96fc9ec6f1a5a', 'width': 238, 'height': 238}, 'resolutions': [{'url': 'https://external-preview.redd.it/RvqZTha_jpsuzZWXfSdwZNHZ_Qvk62R3yl7YvkufrqI.jpg?width=108&crop=smart&auto=webp&s=45348c732a19d9a609ae09564dd3c1bcd96ee24e', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/RvqZTha_jpsuzZWXfSdwZNHZ_Qvk62R3yl7YvkufrqI.jpg?width=216&crop=smart&auto=webp&s=062f6b4f074cf5d7329ced227851ad27f4336d8d', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'TtFVmEs53hWc_tUPscFPQzT0bv04t5CPuG5-SNnw3mo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aos0o3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='c0m94d3'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aos0o3/how_do_i_write_a_data_pipeline_etl/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aos0o3/how_do_i_write_a_data_pipeline_etl/', 'subreddit_subscribers': 160218, 'created_utc': 1707713997.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.705+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've worked as a data professional with different job titles for about 10 years now and I'm noticing a pattern that I haven't seen explored before, and I'm interested in some structure.  I see two types of data engineering roles/teams and I'll try to describe them as best I can in a couple bullets.  \n\n\nType 1: the converted software engineer\n\n* Comfortable in Scala/Rust/Spark/kubernetes\n* Handles not just deployment but serving production systems\n* Highly interested in optimizations as these save the company real money or just to make the problems tractable. \n* More likely to use streaming architectures.\n* Further removed from the business problems.\n\n&#x200B;\n\nType 2: the senior data scientist\n\n* I've built a fancy model, now what?\n* Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well\n* Likely don't have replicated environments.\n* Passes off data artifacts/exposes data products to the SE team for integration into core platform\n* Focused on solving problems for the business, which necessitates data engineering.\n\n&#x200B;\n\nI could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?", 'author_fullname': 't2_xi7dy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering vs Data Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ap40zg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707755384.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve worked as a data professional with different job titles for about 10 years now and I&#39;m noticing a pattern that I haven&#39;t seen explored before, and I&#39;m interested in some structure.  I see two types of data engineering roles/teams and I&#39;ll try to describe them as best I can in a couple bullets.  </p>\n\n<p>Type 1: the converted software engineer</p>\n\n<ul>\n<li>Comfortable in Scala/Rust/Spark/kubernetes</li>\n<li>Handles not just deployment but serving production systems</li>\n<li>Highly interested in optimizations as these save the company real money or just to make the problems tractable. </li>\n<li>More likely to use streaming architectures.</li>\n<li>Further removed from the business problems.</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p>Type 2: the senior data scientist</p>\n\n<ul>\n<li>I&#39;ve built a fancy model, now what?</li>\n<li>Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well</li>\n<li>Likely don&#39;t have replicated environments.</li>\n<li>Passes off data artifacts/exposes data products to the SE team for integration into core platform</li>\n<li>Focused on solving problems for the business, which necessitates data engineering.</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p>I could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ap40zg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='apple_pie_52'), 'discussion_type': None, 'num_comments': 9, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/', 'subreddit_subscribers': 160218, 'created_utc': 1707755384.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.706+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I’m exhausted of the Iceberg vs Delta Lake arguments. They’ve devolved to nitpicking minor features, none of which bring long-term value to Data Engineering.\n\nGoogle and Microsoft appear to be backing OneTable (renamed to XTable). What are your thoughts?\n\nhttps://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065', 'author_fullname': 't2_z15jq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Thoughts on OneTable/XTable?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap2xop', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707752677.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I’m exhausted of the Iceberg vs Delta Lake arguments. They’ve devolved to nitpicking minor features, none of which bring long-term value to Data Engineering.</p>\n\n<p>Google and Microsoft appear to be backing OneTable (renamed to XTable). What are your thoughts?</p>\n\n<p><a href="https://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065">https://cwiki.apache.org/confluence/plugins/servlet/mobile?contentId=276106065#content/view/276106065</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ap2xop', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Data_cruncher'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap2xop/thoughts_on_onetablextable/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap2xop/thoughts_on_onetablextable/', 'subreddit_subscribers': 160218, 'created_utc': 1707752677.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.708+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Thinking of using SQL server, and maybe a web app to access/filter content? Hoping someone has any off the shelf solutions. Thank you.', 'author_fullname': 't2_60f7heoc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What technology would you use if you had a txt extract of a customer data with 16 million rows, and 30 columns and had to make a "user friendly" filtering system?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aopf7d', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707705251.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Thinking of using SQL server, and maybe a web app to access/filter content? Hoping someone has any off the shelf solutions. Thank you.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aopf7d', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Candid94'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aopf7d/what_technology_would_you_use_if_you_had_a_txt/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aopf7d/what_technology_would_you_use_if_you_had_a_txt/', 'subreddit_subscribers': 160218, 'created_utc': 1707705251.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.709+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Since every company is now riding the GenAI bandwagon. How has your work as a DE changed with relation to GenAi? What are some of new changes that have come to your work/ skills expected/ new architecture designs ?', 'author_fullname': 't2_9ckbn9tf2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'GenAI and DE', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aol2vr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707692116.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Since every company is now riding the GenAI bandwagon. How has your work as a DE changed with relation to GenAi? What are some of new changes that have come to your work/ skills expected/ new architecture designs ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aol2vr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Wise_Shop6419'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aol2vr/genai_and_de/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aol2vr/genai_and_de/', 'subreddit_subscribers': 160218, 'created_utc': 1707692116.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.710+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm trying to transition into data engineering and am looking for advice. \n\nIn a few months, I'll graduate with a PhD in physics. I've decided that staying in academia isn't for me, and thought some of my skills might translate well to data engineering. I've been applying to positions for about a month now, and haven't had any luck. I'm seeking feedback. \n\nMy research requires me to use Python to interact with datasets almost daily. I'm confident writing code for data acquisition, complex calculations, as well as data-visualization.\n\nI lack any experience with common cloud-based systems (AWS, Azure, etc). I also don't have SQL experience (although I've been learning on my own time). \n\nAre there any obvious steps I could take to get a foot in the door? Is it worth completing AWS certifications, or am I just throwing money away? \n\nAny advice would be appreciated.", 'author_fullname': 't2_4ebp7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Advice for someone trying to transition into data engineering from academia?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap269w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707750659.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m trying to transition into data engineering and am looking for advice. </p>\n\n<p>In a few months, I&#39;ll graduate with a PhD in physics. I&#39;ve decided that staying in academia isn&#39;t for me, and thought some of my skills might translate well to data engineering. I&#39;ve been applying to positions for about a month now, and haven&#39;t had any luck. I&#39;m seeking feedback. </p>\n\n<p>My research requires me to use Python to interact with datasets almost daily. I&#39;m confident writing code for data acquisition, complex calculations, as well as data-visualization.</p>\n\n<p>I lack any experience with common cloud-based systems (AWS, Azure, etc). I also don&#39;t have SQL experience (although I&#39;ve been learning on my own time). </p>\n\n<p>Are there any obvious steps I could take to get a foot in the door? Is it worth completing AWS certifications, or am I just throwing money away? </p>\n\n<p>Any advice would be appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ap269w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ianmgull'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap269w/advice_for_someone_trying_to_transition_into_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap269w/advice_for_someone_trying_to_transition_into_data/', 'subreddit_subscribers': 160218, 'created_utc': 1707750659.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.712+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'My company is considering migrating from AWS Postgres RDS to either Redshift/Snowflake. \n\nCurrently, we have two ways of ingesting 3rd party providers data:\n\n* Fivetran, which can be connected to both Redshift/Snowflake, \n* and an EC2 instance with Crons, which can be refactored,\n\n I am unsure if a direct connection via the EC2 instance with all the crons there is the best approach for the ingestion. If it fails, we will stop receiving the data from these sources completely.\n\nI was wondering if you could have a better approach for the ingestion or anything to keep in mind on the transition.', 'author_fullname': 't2_126mha', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best approach to 3rd ingest data into Redshift / Snowflake?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoygvj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707739412.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My company is considering migrating from AWS Postgres RDS to either Redshift/Snowflake. </p>\n\n<p>Currently, we have two ways of ingesting 3rd party providers data:</p>\n\n<ul>\n<li>Fivetran, which can be connected to both Redshift/Snowflake, </li>\n<li><p>and an EC2 instance with Crons, which can be refactored,</p>\n\n<p>I am unsure if a direct connection via the EC2 instance with all the crons there is the best approach for the ingestion. If it fails, we will stop receiving the data from these sources completely.</p></li>\n</ul>\n\n<p>I was wondering if you could have a better approach for the ingestion or anything to keep in mind on the transition.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aoygvj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Peivol'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoygvj/best_approach_to_3rd_ingest_data_into_redshift/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoygvj/best_approach_to_3rd_ingest_data_into_redshift/', 'subreddit_subscribers': 160218, 'created_utc': 1707739412.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.713+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\nI\'ve been doing this for circa 4 years now, only building code based solutions.\n\nI\'m chewing on an offer to switch companies (it\'s a good offer and my current employer sucks), and I\'ve been told in the interview that they are considering Dataverse as their data warehouse. \n\nI\'m not well acquainted with the solution, and I couldn\'t find a lot of resources online that would indicate that it\'s used for data warehousing initiatives.\n\nSome factors to consider:\n- I will be the only data engineer, everyone else is business\n- 150 employee company\n- Microsoft shop\n- Very traditional requirements\n- Data volume is very low\n-Very few tables, very few sources\n- For what I\'ve gathered in the interview, the data requirements will not grow that much\n- They considered dataverse because no-one in the company has dev experience, so the "Head of IT" want an easy clickity-click solution\n\nFrom a personal point of view, I don\'t like the fact that I won\'t be able to interact with the database directly. And I won\'t be able to use stored procedures for data transformation (I tend to do ELT), but I will have to rely in dataflow/ADF/logicapps. And also, I guess CICD is out of the question.\n\nAdditionally, I don\'t mind spinning and maintaining the resources I need in Azure (DB instance, VMs for runtime, network, vault, etc), so I can\'t see a single benefit of dataverse from an infra point of view.\n\nI appreciate some out of the box features, but I don\'t need them and I don\'t see them cutting deployment times significantly.\n\nDespite all of the above, I\'m sure I can make it work with Dataverse. So the question is: Why Dataverse would not be a good solution? Does anyone have experience with it? I don\'t want to accept the position, say yes to Dataverse and find out that is not viable.', 'author_fullname': 't2_83poggkq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dataverse', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoenig', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1707699634.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707675829.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>I&#39;ve been doing this for circa 4 years now, only building code based solutions.</p>\n\n<p>I&#39;m chewing on an offer to switch companies (it&#39;s a good offer and my current employer sucks), and I&#39;ve been told in the interview that they are considering Dataverse as their data warehouse. </p>\n\n<p>I&#39;m not well acquainted with the solution, and I couldn&#39;t find a lot of resources online that would indicate that it&#39;s used for data warehousing initiatives.</p>\n\n<p>Some factors to consider:\n- I will be the only data engineer, everyone else is business\n- 150 employee company\n- Microsoft shop\n- Very traditional requirements\n- Data volume is very low\n-Very few tables, very few sources\n- For what I&#39;ve gathered in the interview, the data requirements will not grow that much\n- They considered dataverse because no-one in the company has dev experience, so the &quot;Head of IT&quot; want an easy clickity-click solution</p>\n\n<p>From a personal point of view, I don&#39;t like the fact that I won&#39;t be able to interact with the database directly. And I won&#39;t be able to use stored procedures for data transformation (I tend to do ELT), but I will have to rely in dataflow/ADF/logicapps. And also, I guess CICD is out of the question.</p>\n\n<p>Additionally, I don&#39;t mind spinning and maintaining the resources I need in Azure (DB instance, VMs for runtime, network, vault, etc), so I can&#39;t see a single benefit of dataverse from an infra point of view.</p>\n\n<p>I appreciate some out of the box features, but I don&#39;t need them and I don&#39;t see them cutting deployment times significantly.</p>\n\n<p>Despite all of the above, I&#39;m sure I can make it work with Dataverse. So the question is: Why Dataverse would not be a good solution? Does anyone have experience with it? I don&#39;t want to accept the position, say yes to Dataverse and find out that is not viable.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aoenig', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ancient-Entry-6436'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoenig/dataverse/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoenig/dataverse/', 'subreddit_subscribers': 160218, 'created_utc': 1707675829.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.714+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Curious on thoughts. With the Snowflake release of Dynamic Tables, and the ability to have them update on a schedule, triggered from the last table in the DAG, is there still a place for dbt on Snowflake?\n\nI get that it comes with docs, and a nice viz for lineage, but these things aren’t hard to pull together in other ways.\n\nKeen to hear others thoughts.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is dbt still relevant on Snowflake with Dynamic Tables?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aovhqs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707727196.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Curious on thoughts. With the Snowflake release of Dynamic Tables, and the ability to have them update on a schedule, triggered from the last table in the DAG, is there still a place for dbt on Snowflake?</p>\n\n<p>I get that it comes with docs, and a nice viz for lineage, but these things aren’t hard to pull together in other ways.</p>\n\n<p>Keen to hear others thoughts.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aovhqs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aovhqs/is_dbt_still_relevant_on_snowflake_with_dynamic/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aovhqs/is_dbt_still_relevant_on_snowflake_with_dynamic/', 'subreddit_subscribers': 160218, 'created_utc': 1707727196.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.715+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Column and table names occasionally change, sometimes on tables inside the organization we do not maintain. Our current solution is we have a piece of regex code that finds all of the GitHub repositories that use whatever field or table you are looking for and provides the code that field is in and we tell everyone to update their code based on who usually owns/works on that repo. \n\nMy supervisor doesn't like this because it is manual and wants the DE team to be responsible on fixing things when these changes occur. I don't like writing a regex replace on an entire code base because I find it tedious, prone to error, and prefer to leave coding changes up to the product owner. Am I being stubborn? Are there other ways to manage this?\n\nWe have an on-prem oracle DB, sloowwwlly moving over to snowflake, and code in python/SQL orchestrated by Airflow. \n\n&#x200B;\n\n&#x200B;", 'author_fullname': 't2_f1q7c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Managing SQL Table Changes', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap3flw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707753939.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Column and table names occasionally change, sometimes on tables inside the organization we do not maintain. Our current solution is we have a piece of regex code that finds all of the GitHub repositories that use whatever field or table you are looking for and provides the code that field is in and we tell everyone to update their code based on who usually owns/works on that repo. </p>\n\n<p>My supervisor doesn&#39;t like this because it is manual and wants the DE team to be responsible on fixing things when these changes occur. I don&#39;t like writing a regex replace on an entire code base because I find it tedious, prone to error, and prefer to leave coding changes up to the product owner. Am I being stubborn? Are there other ways to manage this?</p>\n\n<p>We have an on-prem oracle DB, sloowwwlly moving over to snowflake, and code in python/SQL orchestrated by Airflow. </p>\n\n<p>&#x200B;</p>\n\n<p>&#x200B;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ap3flw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='machinegunke11y'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap3flw/managing_sql_table_changes/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap3flw/managing_sql_table_changes/', 'subreddit_subscribers': 160218, 'created_utc': 1707753939.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.716+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm building a data platform and am looking to learn from people who have bought or implemented a new BI tool (Metabase, Looker, Superset, Tableau, anything!) in the last \\~6 months.\n\nI'm more specifically looking to understand what problems you set out to solve, what triggered the decision to adopt a new tool in the first place, what options you looked at, and what made you select that product.\n\nIf you were involved in picking a BI tool recently and are also open to jumping onto a \\~15min call to walk me through your experience, I'd be 1) extremely thankful and 2) also happy to compensate you for your time!", 'author_fullname': 't2_md5ib1c5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How did you choose your BI setup?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoz9ic', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707742134.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m building a data platform and am looking to learn from people who have bought or implemented a new BI tool (Metabase, Looker, Superset, Tableau, anything!) in the last ~6 months.</p>\n\n<p>I&#39;m more specifically looking to understand what problems you set out to solve, what triggered the decision to adopt a new tool in the first place, what options you looked at, and what made you select that product.</p>\n\n<p>If you were involved in picking a BI tool recently and are also open to jumping onto a ~15min call to walk me through your experience, I&#39;d be 1) extremely thankful and 2) also happy to compensate you for your time!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aoz9ic', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AdImaginary8024'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoz9ic/how_did_you_choose_your_bi_setup/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoz9ic/how_did_you_choose_your_bi_setup/', 'subreddit_subscribers': 160218, 'created_utc': 1707742134.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.717+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': " \n\nHi everyone :)\n\nI've been working on a cool project in the past 1.5 months and I was wondering if you'd like to try it. It's called Merlinn, and it's for fellow engineers and anyone who use observability tools. It's an LLM agent designed to speed up incident resolution and minimize the Mean Time to Resolution (MTTR).\n\nWhat it does is it basically connects to your observability tools and data sources and tries to investigate production alerts & incidents on its own, and provide key findings in seconds directly to Slack. You can learn more about it in this website: [https://merlinn.co](https://merlinn.co/)\n\nI'd really love to get some feedback on that and talk about how you investigate and resolve incidents & alerts in your organization. I plan on building more integrations (OpenTelemetry, Prometheus, Google Cloud Logging, etc) and I'd love to talk with the community about observability.", 'author_fullname': 't2_n9em3bcw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I developed a cool new LLM agent that helps with investigating and resolving alerts faster', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoy4oo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707738198.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone :)</p>\n\n<p>I&#39;ve been working on a cool project in the past 1.5 months and I was wondering if you&#39;d like to try it. It&#39;s called Merlinn, and it&#39;s for fellow engineers and anyone who use observability tools. It&#39;s an LLM agent designed to speed up incident resolution and minimize the Mean Time to Resolution (MTTR).</p>\n\n<p>What it does is it basically connects to your observability tools and data sources and tries to investigate production alerts &amp; incidents on its own, and provide key findings in seconds directly to Slack. You can learn more about it in this website: <a href="https://merlinn.co/">https://merlinn.co</a></p>\n\n<p>I&#39;d really love to get some feedback on that and talk about how you investigate and resolve incidents &amp; alerts in your organization. I plan on building more integrations (OpenTelemetry, Prometheus, Google Cloud Logging, etc) and I&#39;d love to talk with the community about observability.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1aoy4oo', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Old_Cauliflower6316'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoy4oo/i_developed_a_cool_new_llm_agent_that_helps_with/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoy4oo/i_developed_a_cool_new_llm_agent_that_helps_with/', 'subreddit_subscribers': 160218, 'created_utc': 1707738198.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.718+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I’m managing a table for streaming data in databricks\nand I want to split it into two: \n1. hot table for the last 7 days\n2. \u2060cold table for historical data. \n\nCurrently, I'm moving data from the hot table to the cold table daily and then deleting it from the hot table. \nThis process is scheduled daily. \n\nIs there a more efficient approach than this?\n\nI can't use the merge option due to the large volume of data", 'author_fullname': 't2_ptil4bof4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to manage Hot and Cold Tables for Streaming Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoue6j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707722542.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I’m managing a table for streaming data in databricks\nand I want to split it into two: \n1. hot table for the last 7 days\n2. \u2060cold table for historical data. </p>\n\n<p>Currently, I&#39;m moving data from the hot table to the cold table daily and then deleting it from the hot table. \nThis process is scheduled daily. </p>\n\n<p>Is there a more efficient approach than this?</p>\n\n<p>I can&#39;t use the merge option due to the large volume of data</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aoue6j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='HousingStriking3770'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoue6j/how_to_manage_hot_and_cold_tables_for_streaming/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoue6j/how_to_manage_hot_and_cold_tables_for_streaming/', 'subreddit_subscribers': 160218, 'created_utc': 1707722542.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi Aspirants,\n\nLast week, I cleared DP-203 with 920 score after 2 weeks of studying. I have made a post about it and the resources used here: [https://www.reddit.com/r/dataengineering/comments/1andkrm/passed\\_dp203\\_last\\_week\\_details\\_for\\_aspirants\\_in/?utm\\_source=share&utm\\_medium=web2x&context=3](https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&utm_medium=web2x&context=3)\n\nThis will be my second and last post related to DP-203. In this post I focus on how you should go about studying as in my preparation I came across so many resources to study but no concrete plan or strategy which only led to confusion.\n\n**Section A: Build Your Knowledge Base**\n\n1. Start with synapse documentation.  \nUnderstand spark pools, difference between serverless and dedicated sql pools. Managed and external tables. Dynamic Data Masking. TDE. Row Level Security and Column Level Security.\n2. Azure Stream Analytics documentation will be up next.  \nBe sure to understand 5 types of windows as atleast 1 question will be on that.\n3. Azure Data Factory Documentation\n4. MS Learn Path  \nSCD Types. 1 questions atleast will come on it.\n\nNote: There is no way you are going to go through all the pages in documentations and next to impossible to remember them. My advise would be to skim through it and try to build a high level knowledge base. You do not need to be expert who knows everything.\n\n**Section B: Real Preparation Starts Here, Build Knowledge required for actual exam.**  \nNote: Below two points are going to be very vital for you to pass the exam as 70-80% questions will be from Dump.\n\n1. Exam Topics free questions.  \nGo through the discussions and the links provided, this is where your best learning is going to happen. There will be some questions with divided answers and you are going to just have to trust your gut. \n2. [https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5](https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5)  \nGo through this video. Many questions will overlap with Exam Topics but still highly recommened watching this video as questions will come from this as well, especially the case study.\n\n**Section C: Important must-do topics**\n\n1. SCD Types: [https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types](https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types)\n2. Replication Zones: [https://youtu.be/K9epl86BGOk](https://youtu.be/K9epl86BGOk)  \nAbove channel is a gold mine for this certification as well as for aspiring Azure Data Engineers. Highly recommend going through his videos, especially security, access tier and ADF ones.\n3. Window Types in Stream Analytics: [https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions](https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions)\n4. Data Masking: [https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql](https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql)\n\n**Section D: Final Tips for Exam**\n\n1. Do give Microsoft practice assessment. \n2. Try to navigate MS learn when studying, giving above assessment, etc. It will be useful when giving exam, you can get couple of answers by going through it.\n3. Know the answers to previously asked questions and the logic for them thoroughly.\n4. Plan how you are going to give exam. I had 41 questions and 100 minutes. I thought I'll be attempting 15 questions every 30 minutes but as it turned out most questions were from dumps so ended up marking 30 questions inside first 20 minutes :)   \nPoint is you will have lot of time if you have done the dumps thoroughly, so mark questions you are not sure about for review, and at the end go through MS learn.  \n\n\nIf anyone has any questions, feel free to reach out to me or comment here. I'll be glad to help.", 'author_fullname': 't2_f86nbjeq2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Study advise for DP-203 Aspirants.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap22x7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707750407.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Aspirants,</p>\n\n<p>Last week, I cleared DP-203 with 920 score after 2 weeks of studying. I have made a post about it and the resources used here: <a href="https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&amp;utm_medium=web2x&amp;context=3">https://www.reddit.com/r/dataengineering/comments/1andkrm/passed_dp203_last_week_details_for_aspirants_in/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>\n\n<p>This will be my second and last post related to DP-203. In this post I focus on how you should go about studying as in my preparation I came across so many resources to study but no concrete plan or strategy which only led to confusion.</p>\n\n<p><strong>Section A: Build Your Knowledge Base</strong></p>\n\n<ol>\n<li>Start with synapse documentation.<br/>\nUnderstand spark pools, difference between serverless and dedicated sql pools. Managed and external tables. Dynamic Data Masking. TDE. Row Level Security and Column Level Security.</li>\n<li>Azure Stream Analytics documentation will be up next.<br/>\nBe sure to understand 5 types of windows as atleast 1 question will be on that.</li>\n<li>Azure Data Factory Documentation</li>\n<li>MS Learn Path<br/>\nSCD Types. 1 questions atleast will come on it.</li>\n</ol>\n\n<p>Note: There is no way you are going to go through all the pages in documentations and next to impossible to remember them. My advise would be to skim through it and try to build a high level knowledge base. You do not need to be expert who knows everything.</p>\n\n<p><strong>Section B: Real Preparation Starts Here, Build Knowledge required for actual exam.</strong><br/>\nNote: Below two points are going to be very vital for you to pass the exam as 70-80% questions will be from Dump.</p>\n\n<ol>\n<li>Exam Topics free questions.<br/>\nGo through the discussions and the links provided, this is where your best learning is going to happen. There will be some questions with divided answers and you are going to just have to trust your gut. </li>\n<li><a href="https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5">https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5</a><br/>\nGo through this video. Many questions will overlap with Exam Topics but still highly recommened watching this video as questions will come from this as well, especially the case study.</li>\n</ol>\n\n<p><strong>Section C: Important must-do topics</strong></p>\n\n<ol>\n<li>SCD Types: <a href="https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types">https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types</a></li>\n<li>Replication Zones: <a href="https://youtu.be/K9epl86BGOk">https://youtu.be/K9epl86BGOk</a><br/>\nAbove channel is a gold mine for this certification as well as for aspiring Azure Data Engineers. Highly recommend going through his videos, especially security, access tier and ADF ones.</li>\n<li>Window Types in Stream Analytics: <a href="https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions">https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions</a></li>\n<li>Data Masking: <a href="https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql">https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql</a></li>\n</ol>\n\n<p><strong>Section D: Final Tips for Exam</strong></p>\n\n<ol>\n<li>Do give Microsoft practice assessment. </li>\n<li>Try to navigate MS learn when studying, giving above assessment, etc. It will be useful when giving exam, you can get couple of answers by going through it.</li>\n<li>Know the answers to previously asked questions and the logic for them thoroughly.</li>\n<li>Plan how you are going to give exam. I had 41 questions and 100 minutes. I thought I&#39;ll be attempting 15 questions every 30 minutes but as it turned out most questions were from dumps so ended up marking 30 questions inside first 20 minutes :)<br/>\nPoint is you will have lot of time if you have done the dumps thoroughly, so mark questions you are not sure about for review, and at the end go through MS learn.<br/></li>\n</ol>\n\n<p>If anyone has any questions, feel free to reach out to me or comment here. I&#39;ll be glad to help.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?auto=webp&s=9bbf148fcd0f0b8b6fd5549050b351928e70b845', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=108&crop=smart&auto=webp&s=cba2396bca6523f443570ee351863f6ce61485eb', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=216&crop=smart&auto=webp&s=92392c8a86e0431966f0e26cb979c564da2e37fd', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/9KjfBI4bExBMA-SppEoXnyUO9JCGHQsyiH1zLRFUuxM.jpg?width=320&crop=smart&auto=webp&s=13b784ad47410ac26c659b9de5295ad6e6f32249', 'width': 320, 'height': 240}], 'variants': {}, 'id': 't-X48R81FbV2hHKHXQHfnFoap_VzKiNwCda90e43_Hs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ap22x7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Vikinghehe'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap22x7/study_advise_for_dp203_aspirants/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap22x7/study_advise_for_dp203_aspirants/', 'subreddit_subscribers': 160218, 'created_utc': 1707750407.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am graduating with a CS degree soon and having a hard time finding a SWE engineering internship. It seems like DE internships are just as competitive. What are some ideas for roles suitable for a CS graduate that enable you to transition to DE? I have been applying to data analyst, business analyst/intelligence, IT internship ect. Not sure what the best path forward to start would be , since I’m striking out with software/data engineering internships as of now. I am trying to find roles that are doable for me to get in this rough market but aren’t dead end roles that won’t be progressing my technical skills to reach one of my end goals (Data engineering, software engineering, cloud engineering). Don’t really have the luxury of applying for a year after graduation, bills have to be paid. If anyone has role suggestions I haven’t listed that would great. Thanks ', 'author_fullname': 't2_126xnl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Viable starting paths to DE?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aouc2b', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1707722769.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707722313.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am graduating with a CS degree soon and having a hard time finding a SWE engineering internship. It seems like DE internships are just as competitive. What are some ideas for roles suitable for a CS graduate that enable you to transition to DE? I have been applying to data analyst, business analyst/intelligence, IT internship ect. Not sure what the best path forward to start would be , since I’m striking out with software/data engineering internships as of now. I am trying to find roles that are doable for me to get in this rough market but aren’t dead end roles that won’t be progressing my technical skills to reach one of my end goals (Data engineering, software engineering, cloud engineering). Don’t really have the luxury of applying for a year after graduation, bills have to be paid. If anyone has role suggestions I haven’t listed that would great. Thanks </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aouc2b', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Kylerhanley'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aouc2b/viable_starting_paths_to_de/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aouc2b/viable_starting_paths_to_de/', 'subreddit_subscribers': 160218, 'created_utc': 1707722313.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Any advice on using Nsx VMware for distributed firewall?', 'author_fullname': 't2_8rpwh5mr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Nsx VMware', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aohyqj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707684144.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Any advice on using Nsx VMware for distributed firewall?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1aohyqj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='OldParticular2326'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aohyqj/nsx_vmware/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aohyqj/nsx_vmware/', 'subreddit_subscribers': 160218, 'created_utc': 1707684144.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.725+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey folks!\n\ndlt (data load tool) library added **Databricks** and **Azure** **Synapse** destinations. Now you too can benefit from schema inference, evolution, management, column level lineage and data contracts.\n\n**How to try?**\n\nSimply run **pip install dlt** and you're halfway there. For the detailed setup, check out the docs:\n\n* [Databricks](https://dlthub.com/docs/dlt-ecosystem/destinations/databricks). Do you already have the dlt “delta live tables” Installed? see [Here](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21)\n* [Azure synapse](https://dlthub.com/docs/dlt-ecosystem/destinations/synapse)\n* Want to try on duckdb? here's a colab [notebook](https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM)\n\n**I'm eager to hear your thoughts:**\n\n* Have you worked with dlt (data load tool) and Databricks before?\n* Did you try running data load tool on databricks but got delta live tables already installed? this [guide](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21) might help\n* How do you currently take data from apis into Databricks or Synapse? Do you use python or something else?\n* Any pain points you think this integration could solve?\n\n**Community-powered**\n\nFinally, shoutout to Evan and his colleagues from [swishbi.com](http://swishbi.com/) for their hard work on the Databricks integration. Collaborations like these are what push the envelope forward in our field.\n\nDo you have some asks from dlt, or interesting stories or use cases you want to tell the world about? Tell us in the  [\\#sharing-and-contributing](https://dlthub.com/community) slack channel\n\nLooking forward to your insights and discussions!", 'author_fullname': 't2_uamr9xer', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'dlt (Data Load Tool) adds Databricks and Azure Synapse destinations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap0ygf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.55, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707747301.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks!</p>\n\n<p>dlt (data load tool) library added <strong>Databricks</strong> and <strong>Azure</strong> <strong>Synapse</strong> destinations. Now you too can benefit from schema inference, evolution, management, column level lineage and data contracts.</p>\n\n<p><strong>How to try?</strong></p>\n\n<p>Simply run <strong>pip install dlt</strong> and you&#39;re halfway there. For the detailed setup, check out the docs:</p>\n\n<ul>\n<li><a href="https://dlthub.com/docs/dlt-ecosystem/destinations/databricks">Databricks</a>. Do you already have the dlt “delta live tables” Installed? see <a href="https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21">Here</a></li>\n<li><a href="https://dlthub.com/docs/dlt-ecosystem/destinations/synapse">Azure synapse</a></li>\n<li>Want to try on duckdb? here&#39;s a colab <a href="https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM">notebook</a></li>\n</ul>\n\n<p><strong>I&#39;m eager to hear your thoughts:</strong></p>\n\n<ul>\n<li>Have you worked with dlt (data load tool) and Databricks before?</li>\n<li>Did you try running data load tool on databricks but got delta live tables already installed? this <a href="https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21">guide</a> might help</li>\n<li>How do you currently take data from apis into Databricks or Synapse? Do you use python or something else?</li>\n<li>Any pain points you think this integration could solve?</li>\n</ul>\n\n<p><strong>Community-powered</strong></p>\n\n<p>Finally, shoutout to Evan and his colleagues from <a href="http://swishbi.com/">swishbi.com</a> for their hard work on the Databricks integration. Collaborations like these are what push the envelope forward in our field.</p>\n\n<p>Do you have some asks from dlt, or interesting stories or use cases you want to tell the world about? Tell us in the  <a href="https://dlthub.com/community">#sharing-and-contributing</a> slack channel</p>\n\n<p>Looking forward to your insights and discussions!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1ap0ygf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thinker_Assignment'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap0ygf/dlt_data_load_tool_adds_databricks_and_azure/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap0ygf/dlt_data_load_tool_adds_databricks_and_azure/', 'subreddit_subscribers': 160218, 'created_utc': 1707747301.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.727+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, i am using aws glue to load big query data into dynamic dataframe but it is taking time. Is there a way to directly load big query data into spark dataframe using glue?', 'author_fullname': 't2_3p3vfvzt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Loading big query data to spark dataframe.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoy0ku', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707737768.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, i am using aws glue to load big query data into dynamic dataframe but it is taking time. Is there a way to directly load big query data into spark dataframe using glue?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aoy0ku', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Dr_Fida'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoy0ku/loading_big_query_data_to_spark_dataframe/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoy0ku/loading_big_query_data_to_spark_dataframe/', 'subreddit_subscribers': 160218, 'created_utc': 1707737768.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.728+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_zrj6c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A Kafka Connect Single Message Transform (SMT) that enables you to append the record key to the value as a named field', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aogoyd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/uX8hbHz46H5hrJgC-dAY9N_fDq5DRCwATkCyQM29h48.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1707680980.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'github.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://github.com/EladLeev/KeyToField-smt', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?auto=webp&s=3de20284b26273a6606b364fff05c750011d095e', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=108&crop=smart&auto=webp&s=5b40d4a2c8cfa1782bf347cb2feeeadf06443bbc', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=216&crop=smart&auto=webp&s=b7f3d119bf1e030b052196dd89ca5a5ee126744c', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=320&crop=smart&auto=webp&s=190dee468e9065a9a140fdff48015c09f3cca672', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=640&crop=smart&auto=webp&s=3d9e755bb1bfb955fa0c383a7b2fb0baaf207d5b', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=960&crop=smart&auto=webp&s=7896542d66d52da9834e58bc4e38378e39fab587', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/aJ-4JNsLhN7t0oyMpXRmjT5fMkwsRx8kpYf8CAw4Ies.jpg?width=1080&crop=smart&auto=webp&s=49d521727ba8409e43c16bb8980f134082dd471c', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'PKDvWqp1Rpki1irwpQ9gcknTnQXYIowvDTsW7KKujSs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1aogoyd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='eladleev'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aogoyd/a_kafka_connect_single_message_transform_smt_that/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://github.com/EladLeev/KeyToField-smt', 'subreddit_subscribers': 160218, 'created_utc': 1707680980.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.729+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am doing a demo and I want to pull data from an API that has recent data. I want to call this API twice and show new data came in. The demo is 5 minutes so I need some API that is refreshing at least once per minute.\n\nAny suggestions for a data source? I am thinking something like weather data etc. The type of data is not super important, just want to show new data points coming in.', 'author_fullname': 't2_vx67of8l', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data API', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aogc7m', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707680103.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am doing a demo and I want to pull data from an API that has recent data. I want to call this API twice and show new data came in. The demo is 5 minutes so I need some API that is refreshing at least once per minute.</p>\n\n<p>Any suggestions for a data source? I am thinking something like weather data etc. The type of data is not super important, just want to show new data points coming in.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1aogc7m', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Hot_Map_7868'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aogc7m/data_api/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aogc7m/data_api/', 'subreddit_subscribers': 160218, 'created_utc': 1707680103.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.730+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'So, company I work for recently transferred patient data to a 3rd party supplier who develop a healthcare management system. We now need to transfer it back to some sort of storage or staging area. We recently got acquisisitioned by another company, and it feels like over kill trying to get the raw data back into some sort of ownership for BI and analytics. What else is there other than encryption in transit, rest, location of data centre, and access controls we need to worry about? To make matters worse the new suppliers design is a generalised one, and the wider business is asking for exact attributes and values, which seems overkill.', 'author_fullname': 't2_va4epm9x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Securing Patient Data in Pharma/Healthcare.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ap5xup', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707759979.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So, company I work for recently transferred patient data to a 3rd party supplier who develop a healthcare management system. We now need to transfer it back to some sort of storage or staging area. We recently got acquisisitioned by another company, and it feels like over kill trying to get the raw data back into some sort of ownership for BI and analytics. What else is there other than encryption in transit, rest, location of data centre, and access controls we need to worry about? To make matters worse the new suppliers design is a generalised one, and the wider business is asking for exact attributes and values, which seems overkill.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ap5xup', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='throwaway_112801'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap5xup/securing_patient_data_in_pharmahealthcare/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap5xup/securing_patient_data_in_pharmahealthcare/', 'subreddit_subscribers': 160218, 'created_utc': 1707759979.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.731+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_vk94wnpj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Rethinking Serverless: The Price of Convenience', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ap58sg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/WeMezRxlZh-HmLTjds5dkbTgXYS6PUKQ2rg5y8H0DIw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1707758293.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/sync-computing/rethinking-serverless-the-price-of-convenience-9b9e29549d3b', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?auto=webp&s=7456914991ca35c39c6c4c487524552911535e37', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=108&crop=smart&auto=webp&s=e7e6d1f2866d127f23d477a6706ac01a53d8b5ba', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=216&crop=smart&auto=webp&s=8f0cb9db0f04dac76165ffa7932072dfa2a0cbea', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=320&crop=smart&auto=webp&s=d2a240378ea1a379b2157b9edb42779cf3d7fa41', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=640&crop=smart&auto=webp&s=7b8f7c6dfe2b750d56671b26d8e3bbcf6ac36243', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=960&crop=smart&auto=webp&s=8336a4eee44066a7ddd6145902604c56732a0784', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/tCZ8jsoDFac886LoJCqbQZFBxsxmbSyqMfJ8tXS0Fuw.jpg?width=1080&crop=smart&auto=webp&s=132d67b274d72a5aea3dc9dcd6d489f8fde715e6', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'kQOOGqlUn0sGgVecv_vx2mlQcdE_rS48JopeFXI6yUg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ap58sg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sync_jeff'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap58sg/rethinking_serverless_the_price_of_convenience/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/sync-computing/rethinking-serverless-the-price-of-convenience-9b9e29549d3b', 'subreddit_subscribers': 160218, 'created_utc': 1707758293.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.732+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello guys, I'm implement right know the databricks at my company, and everything is working as intended, however, I've received a very specific demand from my manager.\nHe wants all the data analysts to be able to see and use every single table while exploring the data, but only use gold tables on powerbi, so, I want all the silver tables to be available for select statements, but I need to find a way to block them to use on the pbi, can this be achieved?\n\nMaybe I'm trying to get this by the wrong way, if there another, better and safer way to do that, I would love to try, but analysts being able to explore silver tables is a must for our company.", 'author_fullname': 't2_glgrj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'There is a way to block a catalog access on databricks if connected to powerbi/tableau?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ap55q1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707758092.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello guys, I&#39;m implement right know the databricks at my company, and everything is working as intended, however, I&#39;ve received a very specific demand from my manager.\nHe wants all the data analysts to be able to see and use every single table while exploring the data, but only use gold tables on powerbi, so, I want all the silver tables to be available for select statements, but I need to find a way to block them to use on the pbi, can this be achieved?</p>\n\n<p>Maybe I&#39;m trying to get this by the wrong way, if there another, better and safer way to do that, I would love to try, but analysts being able to explore silver tables is a must for our company.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ap55q1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='raffapaiva'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap55q1/there_is_a_way_to_block_a_catalog_access_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap55q1/there_is_a_way_to_block_a_catalog_access_on/', 'subreddit_subscribers': 160218, 'created_utc': 1707758092.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.733+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey, I have written a simple stored procedure in python in snowflake. It takes the dictionary in string format and pass it down to a table. It is working fine in snowflake. \n\nBut when I m calling it from power automate, it is giving error something like 'snowpark_temp_stage_ahdjejek'\n\nAt the same time, when I m calling another stored procedure which basically calling it and nothing just return string. Power automate gets success result. \n\nAny one  of you, have faced this issue before!?", 'author_fullname': 't2_1ubfs6x4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Call snowpark stored procedures from power automate', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap379e', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707753361.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey, I have written a simple stored procedure in python in snowflake. It takes the dictionary in string format and pass it down to a table. It is working fine in snowflake. </p>\n\n<p>But when I m calling it from power automate, it is giving error something like &#39;snowpark_temp_stage_ahdjejek&#39;</p>\n\n<p>At the same time, when I m calling another stored procedure which basically calling it and nothing just return string. Power automate gets success result. </p>\n\n<p>Any one  of you, have faced this issue before!?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ap379e', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='asud_w_asud'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap379e/call_snowpark_stored_procedures_from_power/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap379e/call_snowpark_stored_procedures_from_power/', 'subreddit_subscribers': 160218, 'created_utc': 1707753361.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.734+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello all\n\nI currently work a lot with SAP HANA, have done it for 3 years now.\n\nNow that I'm thinking for a switch , I have 2 options...find a SAP HANA DB based job (which won't require a lot of upskilling )\n\nOr else upskill in next 5 months and find a proper data engineer related job\n\nWhat is better financially and growth wise?", 'author_fullname': 't2_6ociw9qb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'SAP HANA jobs', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoxq13', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707736655.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello all</p>\n\n<p>I currently work a lot with SAP HANA, have done it for 3 years now.</p>\n\n<p>Now that I&#39;m thinking for a switch , I have 2 options...find a SAP HANA DB based job (which won&#39;t require a lot of upskilling )</p>\n\n<p>Or else upskill in next 5 months and find a proper data engineer related job</p>\n\n<p>What is better financially and growth wise?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1aoxq13', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Adorable_Finance3027'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoxq13/sap_hana_jobs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoxq13/sap_hana_jobs/', 'subreddit_subscribers': 160218, 'created_utc': 1707736655.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.735+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "So I've been using dbt for over a year by now and i was one of those who requested to have such a Materialization as a feature to be added to dbt\n\nAnd it went out now in the stable release and got documented and that's all great right ! 👍\n\nBut there is this one config that I can't wrap my head around\n\nOn configuration_change= \nApply\nContinue\nFail\n\nWhat are these values mean ? Especially when running things ... I can't debug their behavior, so any ideas what are they inteded to be doing ?", 'author_fullname': 't2_2xhlwgrk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The dbt Materialzed_views Materialization', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aowiur', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707731795.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So I&#39;ve been using dbt for over a year by now and i was one of those who requested to have such a Materialization as a feature to be added to dbt</p>\n\n<p>And it went out now in the stable release and got documented and that&#39;s all great right ! 👍</p>\n\n<p>But there is this one config that I can&#39;t wrap my head around</p>\n\n<p>On configuration_change= \nApply\nContinue\nFail</p>\n\n<p>What are these values mean ? Especially when running things ... I can&#39;t debug their behavior, so any ideas what are they inteded to be doing ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aowiur', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='etsh_96'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aowiur/the_dbt_materialzed_views_materialization/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aowiur/the_dbt_materialzed_views_materialization/', 'subreddit_subscribers': 160218, 'created_utc': 1707731795.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.735+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Why using query builder when you have ORM ? Why Knex when you can have prisma ?\n\nCan you handle big application with query builder only and not ORM ? Is is tedious to not have intellisense when using query builder ? \n\nThanks, you production guys 😎', 'author_fullname': 't2_5lg752r5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ORM vs query builder', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aolwjt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707694360.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Why using query builder when you have ORM ? Why Knex when you can have prisma ?</p>\n\n<p>Can you handle big application with query builder only and not ORM ? Is is tedious to not have intellisense when using query builder ? </p>\n\n<p>Thanks, you production guys 😎</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aolwjt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='colet_te'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aolwjt/orm_vs_query_builder/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aolwjt/orm_vs_query_builder/', 'subreddit_subscribers': 160218, 'created_utc': 1707694360.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.737+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi. Anyone has any experience working in abbott as senior DE. Want to know the work culture or pros/cons working at abbott IL location. \nI have gone through the glass door and indeed but i don’t trust them as negative reviews can be deleted from them.', 'author_fullname': 't2_t0zkmfpy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Experience with Abbott?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aolkgc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707693429.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi. Anyone has any experience working in abbott as senior DE. Want to know the work culture or pros/cons working at abbott IL location. \nI have gone through the glass door and indeed but i don’t trust them as negative reviews can be deleted from them.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1aolkgc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Terrible_Mud5318'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aolkgc/experience_with_abbott/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aolkgc/experience_with_abbott/', 'subreddit_subscribers': 160218, 'created_utc': 1707693429.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.738+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Freshers into Data Engineering, Currently doing course from online EDTECH platform into Data Engineering/science. made one LIVE project.  \nWhat is the hiriing scnerio in Data Analysis is there any hiring Going on or Data Engineering is HYPE only ? ', 'author_fullname': 't2_cmouqp0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Freshers into Data Engineering, Is hiriing going on or not', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ap64p9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.25, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707760439.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Freshers into Data Engineering, Currently doing course from online EDTECH platform into Data Engineering/science. made one LIVE project.<br/>\nWhat is the hiriing scnerio in Data Analysis is there any hiring Going on or Data Engineering is HYPE only ? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ap64p9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ganwaniKamal'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap64p9/freshers_into_data_engineering_is_hiriing_going/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap64p9/freshers_into_data_engineering_is_hiriing_going/', 'subreddit_subscribers': 160218, 'created_utc': 1707760439.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.739+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "hi team,\n\nDE here ;)\n\ni am wondering.\n\nwe were asked to report on some historical data for salesforce tables, however we didnt have any snanspots created properly. we only had 1 that included limited set of columns.\n\nI am wondering: since we have no clue what 'our executives' might wanna know in th future i wonder if it is at all possible to snapshot entire objects (all columns current and future) for tracking history changes?\n\nI was thinking of creating SCD Type 7 all critical salesforce tables.\n\nso we will have \n\n1. raw schema - where we extract data to\n2. snapshot schema - scd type 7 of every table\n\n&#x200B;\n\n**questions**\n\n\\-is it at all easy to maintain ?\n\n\\-anything to keep in mind?\n\n\\-is there any better simpler way ?\n\n&#x200B;\n\n**I am doing it for the very first time.** \n\n&#x200B;\n\nThx\n\n&#x200B;", 'author_fullname': 't2_do9wxbfu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'snowflake - tracking changes', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ap2asm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707750992.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>hi team,</p>\n\n<p>DE here ;)</p>\n\n<p>i am wondering.</p>\n\n<p>we were asked to report on some historical data for salesforce tables, however we didnt have any snanspots created properly. we only had 1 that included limited set of columns.</p>\n\n<p>I am wondering: since we have no clue what &#39;our executives&#39; might wanna know in th future i wonder if it is at all possible to snapshot entire objects (all columns current and future) for tracking history changes?</p>\n\n<p>I was thinking of creating SCD Type 7 all critical salesforce tables.</p>\n\n<p>so we will have </p>\n\n<ol>\n<li>raw schema - where we extract data to</li>\n<li>snapshot schema - scd type 7 of every table</li>\n</ol>\n\n<p>&#x200B;</p>\n\n<p><strong>questions</strong></p>\n\n<p>-is it at all easy to maintain ?</p>\n\n<p>-anything to keep in mind?</p>\n\n<p>-is there any better simpler way ?</p>\n\n<p>&#x200B;</p>\n\n<p><strong>I am doing it for the very first time.</strong> </p>\n\n<p>&#x200B;</p>\n\n<p>Thx</p>\n\n<p>&#x200B;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ap2asm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='87keicam'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ap2asm/snowflake_tracking_changes/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ap2asm/snowflake_tracking_changes/', 'subreddit_subscribers': 160218, 'created_utc': 1707750992.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.740+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7fc747ebbb20>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everybody,\n\nSo maybe I am mistaken but I recently got more into the azure data stack and to me it seems, there are no easy ways to do complex data transformations in azure. Sure you can do data flow actions or notebooks but there are no way to do complex, multi table transformations like for example in dbt. Am I missing something?\n\nCheers', 'author_fullname': 't2_yobj1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Complex data transformations in azure', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1aoj4m3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1707687098.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everybody,</p>\n\n<p>So maybe I am mistaken but I recently got more into the azure data stack and to me it seems, there are no easy ways to do complex data transformations in azure. Sure you can do data flow actions or notebooks but there are no way to do complex, multi table transformations like for example in dbt. Am I missing something?</p>\n\n<p>Cheers</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1aoj4m3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lschozar'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1aoj4m3/complex_data_transformations_in_azure/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1aoj4m3/complex_data_transformations_in_azure/', 'subreddit_subscribers': 160218, 'created_utc': 1707687098.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-02-12T18:20:07.742+0000] {python.py:201} INFO - Done. Returned value was: None
[2024-02-12T18:20:07.766+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20240212T182002, start_date=20240212T182005, end_date=20240212T182007
[2024-02-12T18:20:07.851+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-02-12T18:20:07.891+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
